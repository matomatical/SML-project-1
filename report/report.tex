\documentclass[twocolumn,10pt]{article}

\usepackage[top=2cm,bottom=2cm,left=1.5cm,right=1.5cm]{geometry}

\usepackage{lipsum}

\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\normalsize\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small}
\setlength{\absleftindent}{1em}
\setlength{\absrightindent}{2em}

\usepackage{titlesec}
\titleformat{\section}[block]{\Large\bfseries\filcenter}{}{}{}
% \titleformat{\subsection}[block]{\large\bfseries\filcenter}{}{}{}

\usepackage[dvipsnames]{xcolor}
\usepackage[
    colorlinks=true,
    allcolors=blue % Feel free to change the colour if you prefer another :)
]{hyperref}
\usepackage[
    backend=biber,
    style=numeric-comp,
    sortcites=true,
    giveninits=true,
    hyperref=true,
    sorting=nyt,
    natbib=true,
]{biblatex}
\addbibresource{references.bib}
\renewcommand*{\bibfont}{\footnotesize}
% Get the citations as superscipts (it's like wikipedia!?)
% thanks https://tex.stackexchange.com/questions/114987/biblatex-supercite-with-square-brackets-and-grouped
\DeclareCiteCommand{\supercite}[\mkbibsuperscript]
  {\usebibmacro{cite:init}%
   \let\multicitedelim=\supercitedelim
   \iffieldundef{prenote}
     {}
     {\BibliographyWarning{Ignoring prenote argument}}%
   \iffieldundef{postnote}
     {}
     {\BibliographyWarning{Ignoring postnote argument}}%
  \bibopenbracket}%
  {\usebibmacro{citeindex}%
   \usebibmacro{cite:comp}}
  {}
  {\usebibmacro{cite:dump}\bibclosebracket}
\newcommand{\tocite}{\textsuperscript{[\textcolor{blue}{citation needed}]}}
 
% thanks https://tex.stackexchange.com/a/5255 
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{mathtools}

\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[font=footnotesize]{caption}


\title{COMP90051 Statistical Machine Learning Project 1 Report}
\author{
Alice Johnson,
Marvin Lai,
Matthew Farrugia-Roberts}
\date{Semester 2, 2019}



\begin{document}

\maketitle

\section{Introduction}
For our class project we develop a system for automated
authorship attribution of Twitter messages (`tweets'). We
are given a dataset of 328,932 tweets of known authorship,
and 35,437 anonymous tweets to be attributed to one of
9,297 authors as part of a class Kaggle competition\footnotemark.
\footnotetext{\url{https://www.kaggle.com/c/whodunnit}}

Existing work on authorship attribution of twitter
messages\supercite{rocha2016authorship, bhargava2013stylometric, schwartz2013authorship}
frames the problem as either
(1) supervised multi-class classification, training models on
labelled (known-author) tweets to predict the label (author)
of test tweets, or
(2) an author profiling task, collating all tweets from an
author into a single profile and attributing each anonymous
tweet by finding the closest profile under some distance metric.
Moreover, two broad classes of features are shown to
be effective under these frameworks:
`static' features are hand-crafted features capturing various
stylometric aspects of writing in a given language; and
`dynamic' features are lower level patterns automatically
determined from data.

Our dataset is unique in having an extremely large number
of authors with few training tweets per author:
Over 90\% of our authors have fewer than 50 tweets,
and these tweets make up over 70\% of the dataset.
50 is the fewest tweets-per-author explored in
existing work (to our knowledge).
Multi-class classification algorithms may struggle to
generalise after seeing so few examples for most classes.

After initially promising results from a profile-based
baseline, we elect to focus on deeply exploring profile-based
methods and dynamic features, in the hope that these will
scale more capably to our `extreme' dataset.
In particular, we explore various dynamic feature classes,
coupled with a wide range of profile-based models from
recent literature.
Furthermore, we reformulate existing distance metrics
to make them computationally tractable on our large dataset,
and we introduce new distance metrics of our own design.
% and we introduce a new distance metric of our own design.


\section{Feature classes}
We explore the appropriateness of various dynamic feature
classes for our authorship attribution task, including
character, byte, and word $n$-grams, for $n = 2,3,4,5,6$.

We also explore \emph{flexible pattern} $n$-grams, dynamic
feature classes capturing stylometric information such as
patterns in function-word use\supercite{schwartz2013authorship}.
Flexible patterns are word $n$-grams where words appearing
above a certain frequency in the corpus (`high-frequency words',
or HFWs) are retained, but words appearing below a certain
frequency (`content words', CWs) are conflated. A flexible
pattern $n$-gram is a sequence of $n$ HFWs, each separated
by zero or more CWs.

% old version (did I get all of the important points above?):
% \paragraph{Flexible Patterns} Words are classified as high
% frequency words (HFWs) and/or content words (CWs) based on
% how frequently they appear in the training data. A flexible
% pattern is a sequence of HFWs separated by zero or more CWs\supercite{schwartz2013authorship}.
% We generate the $n$-grams of flexible patterns where CWs
% are replaced by tokens. This captures a pattern such ``the
% $CW$ in the'', which would not have been captured if using
% word $n$-grams\supercite{schwartz2013authorship}.

\paragraph{Pre-processing}
We optionally split tweets at word and punctuation boundaries and
normalise rarely-repeating tokens (e.g. dates, times, numbers)
before extracting $n$-gram features. To examine the effect of
this pre-processing, we also compare to using raw text.

% "To assist with distinguishing between the structure and content of a tweet" was nice, does this fit?
% Sounds much more compact. 

\section{Learners}

We explore several profile-based models for authorship attribution.
Each model defines an author  `profile', and a distance metric $d$
between these profiles and new tweets.
We learn profiles for a set $\mathcal{A}$ of candidate authors from
a corpus of tweets, and then predict the author of each new tweet
$t$ as $\argmin_{a \in \mathcal{A}} d(a, t)$.\footnote{
For the models CNG and SRLP, we additionally handle ties
by selecting the author with the most tweets in the corpus.}
The models are as follows.

\paragraph{Common N-Gram (CNG)}
The CNG model\supercite{kevselj2003n} defines an author's profile
as the normalised frequencies of the $L$ most common $n$-grams
across all of the author's tweets, where $L$ is a hyper-parameter.
$d_{cng}$ measures distance between author $a$ and tweet $t$ as
$$
d_{cng}(a, t) =
    \smashoperator{\sum_{x \in X_a \cup X_t}}\ 
        {\left ( \frac{2 \cdot (P_a(x) - P_t(x))}{P_a(x) + P_t(x)} \right )}^2
$$
where $P_a(x)$ is the normalised frequency of $n$-gram $x$ in
$a$'s profile, $P_t(x)$ is the normalised frequency of $n$-gram
$x$ in $t$, $X_a$ is the set of $n$-grams in author $a$'s profile
(i.e. their $L$ most frequent $n$-grams), and $X_t$ is the set
of $n$-grams in $t$.

This sum over all $n$-grams in $X_a \cup X_t$ is expensive
to compute for every author, for every test tweet.
We exploit the sparsity of our $n$-gram features by using
an equivalent formulation in terms of a sum over only
$X_a \cap X_t$:
$$
d_{cng} (a, t) =
    \smashoperator{\sum_{x \in X_a \cap X_t}}\ 
        {\left ( \frac{2 \cdot (P_a(x) - P_t(x))}{P_a(x) + P_t(x)} \right )}^2
        - 8 \cdot |X_a \cap X_t| + C
$$
where $C=4 \cdot (L + |X_t|)$ is a constant.
We can efficiently compute this sum (and $|X_a \cap X_t|$)
using an inverted index.


\paragraph{Source Code Author Profile (SCAP)} In SCAP\supercite{frantzeskou2006effective}
an author's profile comprises the set of the $L$ most common
$n$-grams across the author's tweets. $d_{scap}$ is then defined in
terms of the overlap of this set with that of the test tweet:
$$
d_{scap}(a, t) = 1 - \frac{|X_a \cap X_t|}{L} % display fraction (looks much nicer)
% d_{scap}(a, t) = 1 - |X_a \cap X_t| / L     % inline fraction (if space needed)
$$

\paragraph{Recentered Local Profile (RLP)} The RLP method\supercite{layton2012recentred}
defines an author's profile as the $L$ $n$-grams that have
the highest absolute `recentered' normalised frequency,
$RP(x) = P(x) - E(x)$ where $P(x)$ is defined as above, and
$E(x)$ is the normalised frequency of $n$-gram $x$ in all tweets.
%\footnote{Conceptually, $E(x)$ approximates the probability
%distribution of $n$-grams over the English language. As the
%probability distribution of $n$-grams over the English language
%is unknown, and the data set is not exclusively in English, we
%approximate $E(x)$ by calculating the normalised frequencies
%of all $n$-grams in the training set.}
$d_{rlp}$ is formulated as a cosine distance over
$X_a \cup X_t$:\footnote{
This is a corrected version of the formulation in \cite{layton2012recentred},
based on \cite{layton2014tutorial}.}
$$
d_{rlp}(a, t) = 1 -
\frac{\displaystyle
    \smashoperator{\sum_{x \in X_a \cup X_t}}
        RP_a(x) \cdot RP_t(x)
}{\displaystyle
    \sqrt{\smashoperator[r]{\sum_{x \in X_a \cup X_t}} RP_a(x)^2
    \cdot \smashoperator{\sum_{x \in X_a \cup X_t}} RP_t(x)^2}
}
$$
As with CNG, this calculation is prohibitively expensive at our scale.
An exact formulation in terms of only $X_a \cap X_t$ is not possible,
so we approXimate RLP (XRLP) instead:
$$
d_{xrlp}(a, t) = 1 -
\frac{\displaystyle
    \smashoperator[r]{\sum_{x \in X_a \cap X_t}}
        RP_a(x) \cdot P_t(x)
    - \smashoperator{\sum_{x \in X_a}}
        RP_a(x) \cdot E(x)
}{\displaystyle
    \sqrt{\smashoperator[r]{\sum_{x \in X_a}} RP_a(x)^2
    \cdot \smashoperator{\sum_{x \in X_t}} RP_t(x)^2}
}
$$
The sum over $X_a \cap X_t$ can be computed using an inverted index,
the sums over $X_a$ are independent of $t$ and can thus be pre-computed,
and the sum over $X_t$ is a constant.

% \paragraph{Simplified RLP (SRLP)}
We also adapt the RLP method further by proposing an
additional, simplified, efficiently computable distance metric
based on the same profiles---Simplified RLP (SRLP):
$$
d_{srlp}(a, t) =
    \smashoperator[l]{\sum_{x \in X_a \cap X_t}}
        \frac{ RP_a(x) } { |RP_a(x)| }
        \cdot
        \frac{ RP_t(x) }{ |RP_t(x) | }
$$

\paragraph{Smooth $P_a$ Cross Entropy (SPaCE)}
We present a new model for profile-based authorship attribution.
SPaCE defines an author profile using the \emph{smoothed}
normalised $n$-gram frequencies from the author's tweets,
including for unseen $n$-grams. 
We interpret these normalised frequencies as a probability
distribution over the set of all $n$-grams, and use the cross
entropy between the probability distributions of $t$ and $a$
(plus a per-author offset) as our distance metric:
$$
d_{space}(a, t) =
    - \ln (P(a)) / N_t
    - \sum_{x \in X_t} P_t(x) \ln (P'_a(x))
$$
$P(a)$ is the proportion of corpus tweets by $a$,
$N_t$ is the total number of $n$-grams in $t$, and
$P'_a(x)$ is the smoothed probability of $x$.
$P'_a(x)$ is defined in terms of $P_a(x)$ using either
(i)   add-$k$ smoothing;
(ii)  linear interpolation with $E(x)$ by $\alpha$; or
(iii) linear interpolation with $E(x)$ by $\exp(-N_a/K)$,
      an amount decaying exponentially with $N_a$,
      the total number of $n$-grams in $a$'s tweets.
$K$, $\alpha$, and $k$ are hyper-parameters.

\paragraph{Ensemble}
We create a simple ensemble in an attempt to combine multiple
dynamic feature classes in a single model.
We use SCAP as a base learner for its computational simplicity,
and attribute tweets to the author selected by the most
base models (i.e. by unweighted relative majority vote).




\section{Experiments}
In this section, we detail our experimental setup for tuning
and comparatively evaluating our learner/feature class
combinations, and report our results.
Since our eventual goal is to attribute each of the unlabelled
tweets with high accuracy, we adopt accuracy as our performance
measure.

\paragraph{Data split}

Aside from through the public leaderboard scores (derived from a
small number of tweets), we can't know the true authors of the
unlabelled tweets ahead of the competition end time.
This means we are unable to effectively use these tweets to compare
the accuracy of different learner/feature class combinations.
In response, \emph{we create our own, larger validation dataset}
by randomly partitioning our labelled dataset in two, as follows:

\begin{itemize}[itemsep=0pt,topsep=0pt]
    % We can collapse this list inline if necessary for space.
    \item \textbf{Validation data}: 69,838 tweets (20\% of labelled data)
    reserved for final evaluation of each learner/feature class combination,
    to be used for final model selection.
    \item \textbf{Reduced training data}: Remaining 259,094 tweets (80\%),
    to be split further for use training and tuning each learner/feature
    class combination.
\end{itemize}


\paragraph{Feature engineering}

Across our experiments with each learner/feature class combination,
we observe
(1) character $n$-grams are most effective for $n=4,5,6$, and with raw
    text input;
(2) byte $n$-grams perform indistinguishably from character $n$-grams;
(3) word $n$-grams perform best for $n=2$, where they benefit slightly
    from our pre-processing; and
(4) flexible pattern $n$-grams perform best with $n=2,3$.
Figure~\ref{fig:features} exemplifies these relationships.
For brevity, we report only results from learners trained on these
high character $n$-grams and low word/flexible pattern $n$-grams below.

\begin{figure}[h]
    \centering
    \includegraphics[
        width=\columnwidth,
        trim=0 2ex 0 1.6ex,
        clip
    ]{report/effect-of-n.pdf}
    \caption{Effect of pre-processing and $n$ on SCAP, fixed $L=300$.}
    \label{fig:features}
\end{figure}

% plot of SCAP accuracy vs. n for various levels, normalisation

\paragraph{Tuning hyper-parameters}

For each learner/feature class combination, we tune our hyper-parameter
using grid search optimisation on the reduced training data, as follows:

We tune the profile length $L$ for SCAP using an 8-fold cross-validated
grid search over the reduced training data.

Evaluating each configuration of CNG, XRLP, and SRLP is more
computationally expensive, so for these models we tune $L$ using
holdout validation rather than 8-fold cross validation
(we train on 87.5\% of the reduced training data, and select the $L$
giving the highest accuracy on the other 12.5\%).

SPaCE models are our most computationally expensive to evaluate,
since smoothing removes the sparsity of profiles.
To tune the hyper-parameter for each smoothing method
($k$ for method i, $\alpha$ for method ii, and $K$ for method iii)
we perform a grid search using holdout validation on the reduced training
data, evaluating on 1000 tweets (0.3\%).
Due to time constraints, we only tune on character $n$-grams.

For our ensemble, we try various combinations of features, and use
holdout validation to select the best combination
(seven SCAP base models using character 2--6-grams, word 2-grams,
and flexible pattern 2-grams, respectively).


\paragraph{Model selection}
After turning each learner/feature class combination as above,
we re-train the tuned configurations on the entire reduced
training set, and measure accuracy with our validation data.
Table \ref{tab:devresults} summarises our results.

\begin{table}[h]
\centering
\begin{tabular}{@{}cccccccc@{}}
\cmidrule{3-8}
Acc. (\%)   &     & \multicolumn{6}{c}{Feature class}                    \\
\cmidrule{3-8} 
            &     & \multicolumn{3}{c}{character} & word  & \multicolumn{2}{c}{flex. patt.} \\
\cmidrule(r){1-1} \cmidrule(rl){3-5} \cmidrule(rl){6-6} \cmidrule(rl){7-8} 
Models      & $n$ & 4      & 5      & 6           & 2     & 2     & 3    \\
\cmidrule(r){1-1} \cmidrule{3-8} 
CNG         &     & 25.4   & 26.2   & 26.4        & 20.5  & 13.8  & 12.5 \\
SCAP        &     & 21.7   & 22.2   & 22.1        & 14.4  &  9.7  &  8.5 \\
XRLP        &     & 18.6   & 18.4   & 17.4        & 12.4  &  9.4  & 10.0 \\
SRLP        &     & 23.7   & 25.4   & 25.7        & 18.3  & 12.5  & 11.9 \\
SPaCE i     &     & 27.5   & 28.3   & 28.0        & ---   & ---   & ---  \\
SPaCE ii    & &{\bf 32.7}  & 32.2   & 30.9        & ---   & ---   & ---  \\
SPaCE iii   &     & 32.5   & 31.5   & 30.1        & ---   & ---   & ---  \\
                  \cmidrule{3-8} 
Ensemble    &     & \multicolumn{6}{c}{ ---23.4--- }\\ % (char23456grams+word2grams+flex2grams)
\bottomrule
\end{tabular}
\caption{Tuned model accuracy on held-out validation data.}
\label{tab:devresults}
\end{table}

We re-train our best performing model/feature class combination
(SPaCE ii, character 4-grams) on the entire labelled dataset for submission,
achieving a public score of \textbf{34.7\%}.

\section{Critical analysis}

% Matt: Should we leaf this section with the SPaCE discussion?
% Since it's our strongest model.

% Compare local profile methods and compare to results found in literature - Alice
\paragraph{CNG, SCAP, RLP} Recent works show RLP outperforms CNG on large documents with few authors\supercite{layton2012recentred}, and SCAP outperforms CNG when there is limited training data per author\supercite{frantzeskou2006effective}. Our results show CNG outperforming both SCAP and RLP. We attribute these contrasting findings to dataset differences---RLP's measure of distinctiveness is % <-- Matt: 'may be'?
less effective when there is limited training data per author because there is less data to compare distinctiveness. % <-- Matt: This seems a little vague 
CNG and SCAP are comparable in performance until an author's profile becomes smaller than $L$\supercite{frantzeskou2006effective}.
% Our implementation overcomes this issue by padding an authors profile with unmatchable $n$-grams until it is at least L in length.
% Matt: ^^ I think this is not quite true of our implementation (as far as I followed it)
%          maybe the following sentence instead?
Our reformulation overcomes this issue by correcting for profile length, as if to pad out an authors profile with $n$-grams until it has length $L$.
This modification may help explain CNG's effectiveness.

% Matt: NOTE! I am grouping together the discussion of ngrams and
%             ensemble improvement into a single labelled paragraph:
\paragraph{Features}
% compare character grams versus word and flex grams
% compare features. - Marvin
% \paragraph{$n$-grams}
Previous work\supercite{layton2010authorship} finds that character 4-grams
are the most effective feature class for Twitter data, with a drop in
accuracy for $n<4$ and $n>4$.
The study considered only around 50 authors (cf. our 9,297). 
Our tests show that increasing $n$ tends to increase accuracy. 
% Matt: ^^ Could we say 'Our tests show that character 5-grams are more effective overall.'? 
%          Just because that's what the evaluation data we have included seems to say

% The discussion shifts into a new idea here, so I'm inserting a blank line/paragraph break.
% Matt: vv I think this overlaps considerably with the next paragraph on ensemble result
%          perhaps we can remove this, or merge?
Word and flexible pattern $n$-grams perform worse in isolation than character $n$-grams.
% there's no talk on what word and flex actually represent about the text
While not the most effective alone, using a combination of these features simultaneously can improve accuracy\supercite{rocha2016authorship}.
However, the memory and time required to do so make it infeasible on any of our machines.
% Matt: ^^ This doesn't seem super relevant
% Matt: vv Are these other suggestions to talk about?
% Character $n$-grams consistently outperform word and flexible pattern $n$-grams. 
% % Different levels of $n$-gram captures different information regarding an author's writing habits.
% Increasing $n$ for character $n$-grams tend to produce more accurate results (except in SPaCE).
% This contrasts studies which find that $n=4$ was the most effective parameter for their case\supercite{rocha2016authorship}. % actually it's from a citation in that citation which said n=4 is best
% The studies differ by having few authors - around 50 - whereas we have over 9,000. % IT'S OVER 9000!
% Combining the studies and our tests suggest that longer character $n$-grams are required as the number of authors increase.
% % Longer $n$-grams are potentially more specific to an author, as there is more variety of $n$-grams and less likely to be shared between authors.
% % However doing so also assumes that authors do repeat relatively long sections of text, otherwise learned $n$-grams will never appear in the test data. % learned? common? $n$-grams seen in training?
% % what do I say about words and flex
% Word $n$-grams performing consistently worse than character $n$-grams suggest that semantic information of a text is not as important as the raw information itself.
% % Matt: 'raw information' is quite vague, what do you mean?
% %       we also discussed mentioning the following points on Tuesday night:
% %       % flex patterns v. words: trading off structure/content.
% %       % maybe in this dataset content is more reliable.
% %       % words v. chars: perhaps the lower density of words
% %       % (fewer n-grams per tweet) exacerbates our lack-of-data-
% %       % per-author problem, crippling these profile-based methods
% %       % (profiles are unreliable without much data).
% % Longer character $n$-grams can in a sense substitute shorter word $n$-grams by capturing the $n$-grams that span multiple words while also capturing word uni-grams (1-gram).
% Flexible pattern $n$-grams performed the worse of all features.
% As it reflects the structure of a text rather than the contents, it may perform more effectively at longer texts. 
% However, our data comprises of texts with limited length, as such distinguishing structures may not emerge. 
% % However, with the given data we are dealing with the 140 character limit of a Tweet, there is not much room for distinguishing structure to emerge. % does that make sense? Do we need to justify why it didn't work?
% Matt: I don't think we need to justify why it didn't work. I think there's probably
%       room for 'distinguishing structure' in tweets, especially through use of
%       punctuation. but like we said the other night, maybe content (words) are more
%       reliable overall in this dataset. it might also be the density problem (fewer
%       words/flex pattern ngrams fit in a tweet than character ngrams do).

% Combining the features in the ensemble performed better than word and flexible pattern by itself. Suggests ... orthogonal? - Shared
% \paragraph{Ensemble}
Character, word and flexible pattern $n$-grams are prone to
error because of noise in the data.
Combining the features in the ensemble model captures more
information about authorship, suggesting that these features
are orthogonal.
Future work may investigate more sophisticated methods for
combining features into a more robust model. 

% Matt: How about:
% Combining multiple feature classes working at different `levels'
% within tweets in our SCAP ensemble outperformed each individual
% feature class. This suggests that different classes are capturing
% orthogonal aspects of `authorship'.
% Future work may investigate more sophisticated methods for
% combining features into a robust model.

% SPaCE is better than all profile methods. Why? - Matt
\paragraph{SPaCE}
We observe that SPaCE models outperform all other models for
character-level $n$-grams.
Minimising $d_{space}$ corresponds to maximising
tweet (log) likelihood assuming tweets are sequences of
$n$-grams drawn independently from their author's $n$-gram
probability distribution.
It's somewhat surprising to see this level of performance,
given the naivety of this assumption.
However, SPaCE use a much richer profile representation
than other methods, with smoothing providing effective
regularisation. This may help it to make finer grained
distinctions between authors.

The choice of smoothing method is critical.
We see additive smoothing (i) outperformed by interpolation
smoothing (ii, iii).
Additive smoothing corresponds to assuming a uniform prior
on $n$-gram probability distributions when estimating profiles,
while interpolation methods take corpus-level frequency
information into account.
Since authors' $n$-gram distribution are indeed highly
non-uniform, interpolation smoothing is theoretically
more appropriate. % TODO: Reword this last sentence.


\printbibliography

\end{document}